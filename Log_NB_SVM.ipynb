{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Log_NB_SVM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b6uNVP3Ec0j"
      },
      "source": [
        "# Fake/Real news dataset\n",
        "### Завантажити датасет можна за наступним посиланням:  \n",
        "> https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset/download\n",
        "\n",
        "### Загалом датасет має наступний вигляд: \n",
        "\n",
        "```Index(['title', 'text', 'subject', 'date'], dtype='object')```\n",
        "\n",
        "Для подальшої обробки було додано колонку 'Authenticity' та було об'єднано датасети в один news_data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ciiWEBgE9sm",
        "outputId": "a633c81f-6990-4e5d-a483-bb7158dfad99"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "real = pd.read_csv(\"True.csv\")\n",
        "fake = pd.read_csv(\"Fake.csv\")\n",
        "\n",
        "print(real.columns)\n",
        "fake['Authenticity'] = 0\n",
        "real['Authenticity'] = 1\n",
        "print(real.columns)\n",
        "\n",
        "news_data = fake.append(real)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['title', 'text', 'subject', 'date'], dtype='object')\n",
            "Index(['title', 'text', 'subject', 'date', 'Authenticity'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYlcyrMGI-Tn"
      },
      "source": [
        "### Далі конветуємо колекцію нашого тексту у матрицю токенів"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KT_EQOofPOS"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "valid_data = shuffle(news_data)\n",
        "\n",
        "countvec = CountVectorizer()\n",
        "count_dat = countvec.fit_transform(valid_data['text'].astype('str'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1O9cVvaJXJU"
      },
      "source": [
        "###Ділимо датасет на тренувальний та валідаційний"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGQUkUbrfTiy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(count_dat, valid_data['Authenticity'], test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBwtJOlLJ2Gw"
      },
      "source": [
        "###Логістична регресія для CountVectorizer виглядає наступним чином"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y13tkiKmfVgJ",
        "outputId": "9b2a3234-12b7-4633-9cea-029ff708169e"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "logreg = LogisticRegression(solver=\"liblinear\")\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_valid)\n",
        "accuracy = accuracy_score(y_valid, y_pred)\n",
        "print(\"Logistic regression accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression accuracy:  0.9960653303637713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6roCVwM7KZWv"
      },
      "source": [
        "###Наївний Баєс для CountVectorizer виглядає наступним чином"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qoEPQc6fXGp",
        "outputId": "ff438b67-9a2f-4950-f78a-551719f74305"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "y_pred = mnb.predict(X_valid)\n",
        "\n",
        "accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "print(\"NB accurracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB accurracy:  0.9529324424647364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2tsVVIQKd4F"
      },
      "source": [
        "###SVC для CountVectorizer виглядає наступним чином"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClKrSP0vfZUy",
        "outputId": "fa8b911e-b8d1-463e-c0d2-1f353073f8f3"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC(max_iter=100000)\n",
        "svm.fit(X_train, y_train)\n",
        "svm_acc = svm.score(X_valid, y_valid)\n",
        "print(\"SVM accuracy: \", svm_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM accuracy:  0.9956941351150705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vhy2ZeMKi1W"
      },
      "source": [
        "###Далі пробуємо замість CountVectorizer використати TfidfVectorizer для токенізації нашого текту"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWYlPhF5FhYk"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "valid_data = shuffle(news_data)\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_text = tfidf.fit_transform(valid_data['text'].astype('str'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flveXLa2Ky1l"
      },
      "source": [
        "###Знову ділимо датасет на тренувальні та валідаційні дані"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYyj0JCyFZ0N"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_text, valid_data['Authenticity'], test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXB1E2C7K6yC"
      },
      "source": [
        "###Логістична регресія для TfidfVectorizer виглядає наступним чином"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lta93hccFmMj",
        "outputId": "dcab4611-a0e8-498d-9b9f-942da0579588"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "logreg = LogisticRegression(solver=\"liblinear\")\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_valid)\n",
        "accuracy = accuracy_score(y_valid, y_pred)\n",
        "print(\"Logistic regression accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression accuracy:  0.9868596881959911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOB9TxVqK_LT"
      },
      "source": [
        "###Далі представлена логістична регресія з динамічним підбором гіперпараметрів\n",
        "\n",
        "Наступна стрічка відповідає за перебір гіперпараметрів від (0.001 до 10^10)\n",
        "> np.logspace(-3, 10, 24) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FoG1t7u1H77",
        "outputId": "c057066c-4834-440f-bf7a-f586c11ac12e"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "  \n",
        "c_space = np.logspace(-3, 10, 24)\n",
        "\n",
        "param_grid = {\n",
        "    'C': c_space\n",
        "    }\n",
        "  \n",
        "logreg_cv = GridSearchCV(LogisticRegression(max_iter=10000), param_grid, cv = 5)\n",
        "  \n",
        "logreg_cv.fit(X_train, y_train)\n",
        "  \n",
        "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
        "print(\"Best score is {}\".format(logreg_cv.best_score_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tuned Logistic Regression Parameters: {'C': 14924955.45051826}\n",
            "Best score is 0.9665251436600079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwv0FD7XL0d5"
      },
      "source": [
        "###Наївний Баєс для TfidfVectorizer виглядає наступним чином"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwLRoyJiFoEb",
        "outputId": "45756bd8-f5a5-44e9-add9-a7f2f8fb9333"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "y_pred = mnb.predict(X_valid)\n",
        "\n",
        "accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "print(\"NB accurracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB accurracy:  0.9364513734224202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAKwdvwmL4_v"
      },
      "source": [
        "###SVC для TfidfVectorizer виглядає наступним чином"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3FciojaFp36",
        "outputId": "e1e998a0-8b5e-466f-a5b5-80141a9a4d8a"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC(max_iter=10000)\n",
        "svm.fit(X_train, y_train)\n",
        "svm_acc = svm.score(X_valid, y_valid)\n",
        "print(\"SVM accuracy: \", svm_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM accuracy:  0.9933927245731254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUKviedEMFX-"
      },
      "source": [
        "### Токенізація, яка повертає список оброблених токенів"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06oSnqzdFu76"
      },
      "source": [
        "import spacy\n",
        "from typing import List\n",
        "\n",
        "\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "\n",
        "  disable = [\"ner\", \"parser\", \"tagger\", \"lemmatizer\", \"tok2vec\", \"senter\", \"attribute_ruler\"]\n",
        "  tokens = spacy_nlp(text, disable=disable)\n",
        "  return [str(token).lower() for token in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XegEHhWNMJsW"
      },
      "source": [
        "### Glove\n",
        "Завантажуємо 50к найбільш уживаних слів з Glove."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxyciOQVF0r7"
      },
      "source": [
        "import gensim.downloader\n",
        "from gensim.models import KeyedVectors\n",
        "import tqdm\n",
        "\n",
        "def load_glove_subset(max_n: int) -> KeyedVectors:\n",
        "    all_glove = gensim.downloader.load(\"glove-wiki-gigaword-200\")\n",
        "    subset = KeyedVectors(all_glove.vector_size)\n",
        "    for word in tqdm.tqdm(all_glove.vocab, total=max_n):\n",
        "        if len(subset.vectors) >= max_n:\n",
        "            break\n",
        "        subset.add(word, all_glove[word])\n",
        "    return subset\n",
        "\n",
        "glove = KeyedVectors.load(\"http://134.209.248.229:8081/glove-50k.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN7-HTA4OrIl"
      },
      "source": [
        "###Функція bag_of_embeddings використовує раніше написаний токенізатор та перебирає слова з датасету glove. Оскільки тексти є різних розмірів, тому ми використовуємо лише медіанне значення для кожного тексту."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAXuniIJF2zM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def bag_of_embeddings(dataset):\n",
        "    X = []\n",
        "    for index, doc in dataset.iterrows():\n",
        "        tokens = tokenize(doc['text'])\n",
        "        token_vectors = []\n",
        "        for token in tokens:\n",
        "            if token in glove:\n",
        "                token_vectors.append(glove[token])\n",
        "                \n",
        "        if token_vectors:\n",
        "          doc_vector = np.array(token_vectors).mean(axis=0)\n",
        "        if not isinstance(doc_vector, float):\n",
        "          X.append(doc_vector)\n",
        "        else :\n",
        "          dataset.drop(dataset.index[index])\n",
        "\n",
        "    X = np.stack(X)\n",
        "    y = np.array(dataset['Authenticity'])\n",
        "    return (X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRoRXm3oN67q"
      },
      "source": [
        "###Далі розділяємо наш датасет на тренувальні та валідаційні дані та застосовуємо bag_of_embeddings() для токенізації."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IBv5_IqF6GC",
        "outputId": "06f84c8f-57c1-42f9-ca72-0707be7ea266"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "\n",
        "valid_data = shuffle(news_data)\n",
        "\n",
        "validation = []\n",
        "training = []\n",
        "for i in range(len(valid_data)):\n",
        "  if i >= 2000:\n",
        "    training.append(valid_data.iloc[i])\n",
        "  else:\n",
        "    validation.append(valid_data.iloc[i])\n",
        "\n",
        "valid_data = pd.DataFrame (validation, columns=['title', 'text', 'subject', 'date', 'Authenticity'])\n",
        "train_data = pd.DataFrame (training, columns=['title', 'text', 'subject', 'date', 'Authenticity'])\n",
        "\n",
        "#for index, item in train_data.iterrows():\n",
        "#  print(type(item[\"text\"]))\n",
        "print(len(valid_data))\n",
        "print(len(train_data))\n",
        "X_train, y_train = bag_of_embeddings(train_data)\n",
        "print(len(valid_data))\n",
        "print(len(train_data))\n",
        "X_valid, y_valid = bag_of_embeddings(valid_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "42898\n",
            "2000\n",
            "42898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJVVfVw_OMMZ"
      },
      "source": [
        "###Логістична регресія для bag_of_embeddings виглядає наступним чином"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HvSQXriGANc",
        "outputId": "87470c23-503e-435f-f4e0-35c4a468bc5d"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "logreg = LogisticRegression(solver=\"liblinear\")\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_valid)\n",
        "accuracy = accuracy_score(y_valid, y_pred)\n",
        "print(\"Logistic regression accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression accuracy:  0.9585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6NC9A9zOUQp"
      },
      "source": [
        "###Наївний Баєс для bag_of_embeddings виглядає наступним чином"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p3oddtYGCZu",
        "outputId": "2a550ecb-d3ad-41b0-aa9f-ae7584984106"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_valid)\n",
        "\n",
        "accuracy = accuracy_score(y_valid, y_pred)\n",
        "\n",
        "print(\"NB accurracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB accurracy:  0.8715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLn_2LSaOZjP"
      },
      "source": [
        "###SVC для bag_of_embeddings виглядає наступним чином"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5zHnb3nGDzm",
        "outputId": "dcc391c8-a104-4c71-9374-38a2a89d11bd"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC(max_iter=10000)\n",
        "svm.fit(X_train, y_train)\n",
        "svm_acc = svm.score(X_valid, y_valid)\n",
        "print(\"SVM accuracy: \", svm_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM accuracy:  0.963\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}